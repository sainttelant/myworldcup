{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec6dd9fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is begin to crawl the year of 1999 's data'\n",
      "200\n",
      "it is begin to crawl the year of 2001 's data'\n",
      "200\n",
      "it is begin to crawl the year of 2004 's data'\n",
      "200\n",
      "it is begin to crawl the year of 2007 's data'\n",
      "200\n",
      "it is begin to crawl the year of 2011 's data'\n",
      "200\n",
      "it is begin to crawl the year of 2015 's data'\n",
      "200\n",
      "it is begin to crawl the year of 2016 's data'\n",
      "200\n",
      "it is begin to crawl the year of 2019 's data'\n",
      "200\n",
      "it is begin to crawl the year of 2021 's data'\n",
      "200\n",
      "206 206 206 206 206 206\n",
      "      HomeTeam  AwayTeam  Year HomeGoals AwayGoals  TotalGoals\n",
      "0     Paraguay   Bolivia  1999         0         0           0\n",
      "1         Peru     Japan  1999         3         2           5\n",
      "2     Paraguay     Japan  1999         4         0           4\n",
      "3         Peru   Bolivia  1999         1         0           1\n",
      "4      Bolivia     Japan  1999         1         1           2\n",
      "..         ...       ...   ...       ...       ...         ...\n",
      "201  Argentina   Ecuador  2021         3         0           3\n",
      "202     Brazil      Peru  2021         1         0           1\n",
      "203  Argentina  Colombia  2021         1         1           2\n",
      "204   Colombia      Peru  2021         3         2           5\n",
      "205  Argentina    Brazil  2021         1         0           1\n",
      "\n",
      "[206 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "\n",
    "proxies = {\n",
    "    'https': 'https://127.0.0.1:1080',\n",
    "    'http': 'http://127.0.0.1:1080'\n",
    "}\n",
    "# 需要加上headers， 否则报错: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 8974: invalid start byte\n",
    "\n",
    "headers ={\n",
    "        'accept-language': 'zh-CN,zh;q=0.9',\n",
    "        'referer': 'https://en.wikipedia.org/wiki/UEFA_European_Championship',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "   \n",
    "}\n",
    "home = []\n",
    "away = []\n",
    "homegoal =[]\n",
    "awaygoal=[]\n",
    "totalgoal=[]\n",
    "Year=[]\n",
    "aa=[]\n",
    "bb = []\n",
    "\n",
    "def crawleurocup(year):\n",
    "    for years in year:\n",
    "        print(\"it is begin to crawl the year of %s 's data'\"%(years))\n",
    "        google_url = 'https://en.wikipedia.org/wiki/{}_Copa_Am%C3%A9rica'.format(years)\n",
    "        opener = request.build_opener(request.ProxyHandler(proxies))\n",
    "        request.install_opener(opener)\n",
    "\n",
    "        req = request.Request(google_url, headers=headers)\n",
    "        response = request.urlopen(req)\n",
    "        print(response.status)\n",
    "        content = soup(response,\"lxml\")\n",
    "        \n",
    "        #score \n",
    "        \"\"\"\n",
    "        for info in content.select(\".fscore\"):\n",
    "            bb.append(info.get_text())\n",
    "        \"\"\"\n",
    "        \n",
    "        for info_guo in content.select(\".fhome\"):\n",
    "            #print(info_guo)\n",
    "            \n",
    "            home.append(info_guo.find(\"a\").get_text())\n",
    "        \n",
    "        for info_away in content.select(\".faway\"):\n",
    "            away.append(info_away.find(\"a\").get_text())\n",
    "            \n",
    "        \"\"\"\n",
    "        for c in aa:\n",
    "            guojia=c.split(\"#\")[1]\n",
    "            #print(guojia)\n",
    "            try:\n",
    "                away.append(guojia.split(\"_\")[2])\n",
    "            except:\n",
    "                away.append(\"finalaway\")\n",
    "            home.append(guojia.split(\"_\")[0])\n",
    "        \"\"\"\n",
    "        for goals in content.select(\".fscore\"):\n",
    "            goals = goals.get_text()\n",
    "            #print(goals.split(\"–\")[0])\n",
    "            homegoal.append(goals.split(\"–\")[0])\n",
    "            Year.append(years)\n",
    "            awayg = goals.split(\"–\")[1]\n",
    "            if \"(a.e.t.)\" in awayg:\n",
    "                #print(\"there is aet in there!\")\n",
    "                awayg=awayg.replace(\" (a.e.t.)\",\"\")\n",
    "            if \"(a.e.t./g.g.)\" in awayg:\n",
    "                print(\"there is g.g. in there!\")\n",
    "                print(awayg)\n",
    "                awayg = awayg.replace(\" (a.e.t./g.g.)\",\"\")\n",
    "            awaygoal.append(awayg)\n",
    "            totalgoal.append(int(goals.split(\"–\")[0])+int(awayg))\n",
    "        \"\"\"\n",
    "        for final in content.select(\".fhome\"):\n",
    "\n",
    "            fina=final.find(\"a\").get_text()\n",
    "            home.append(fina)\n",
    "        for aways in content.select(\".faway\"):\n",
    "            awayy = aways.find(\"a\").get_text()\n",
    "            away.append(awayy)\n",
    "        \"\"\"\n",
    "        #home.append(\"finalhome\")\n",
    "        #away.append(\"finalway\")\n",
    "    \n",
    "    #print(len(bb))\n",
    "    pd_name = {\"HomeTeam\":home, \"AwayTeam\":away,\"Year\":Year,\"HomeGoals\":homegoal,\"AwayGoals\":awaygoal,\"TotalGoals\":totalgoal}\n",
    "    \n",
    "    print(len(home),len(away),len(homegoal),len(Year),len(awaygoal),len(totalgoal))\n",
    "    df =pd.DataFrame(pd_name)\n",
    "    print(df) \n",
    "    return df\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \"\"\"\n",
    "    print(\"begin to crawl copa soccer game of america!\")\n",
    "    url = \"https://en.wikipedia.org/wiki/Copa_Am%C3%A9rica\"\n",
    "    opener = request.build_opener(request.ProxyHandler(proxies))\n",
    "    request.install_opener(opener)\n",
    "    req = request.Request(url, headers=headers)\n",
    "    response = request.urlopen(req)\n",
    "    if response.status==200:\n",
    "        print(\"it has climbed over the great wall of China!! \")\n",
    "    else:\n",
    "        print(\"you has been blocked! no hope<<<<exit\")\n",
    "        exit()\n",
    "    content = soup(response,\"lxml\")\n",
    "   \n",
    "    for all_years in content.select(\"td a\"):\n",
    "        print(all_years.get_text())\n",
    "        #print(all_years.find(\"a\")[\"href\"])\n",
    "    \"\"\"\n",
    "    \n",
    "    yearr = ['1999','2001','2004','2007','2011','2015','2016','2019','2021']\n",
    "    #yearr = [\"2016\",\"2019\"]\n",
    "    euromatch = crawleurocup(yearr)\n",
    "    euromatch.to_csv(\"data/copa.csv\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb790096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
